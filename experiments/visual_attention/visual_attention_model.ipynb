{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple object recognition with visual attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebbook, I implement the network from Ba et al. \"Multiple object recognition with visual attention\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlimpseNet(object):\n",
    "  \"\"\"Glimpse network.\n",
    "  \"\"\"\n",
    "  def __init__(self, \n",
    "               layer_size=(\n",
    "                 (3, 3, 1, 64),\n",
    "                 (3, 3, 64, 64),\n",
    "                 (3, 3, 64, 64),\n",
    "                 (16 * 16 * 64, 128)\n",
    "               )):\n",
    "    \"\"\" \n",
    "    \n",
    "    Args:\n",
    "      layer_size: layer sizes (tuple of shape tuples).\n",
    "        default:\n",
    "          layer 1: 3x3 conv, input with 1 color channel, 64 channels output\n",
    "          layer 2: 3x3 conv, 64 channels output\n",
    "          layer 3: 3x3 conv, 64 channels output\n",
    "          layer 4: fully connected, input with size 16x16, output size 128\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    self.params = {}\n",
    "    with tf.variable_scope(\"glimpse_net\"):\n",
    "      self.params[\"W_conv1\"] = tf.get_variable(name=\"W_conv1\", shape=layer_size[0],\n",
    "                          initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "      self.params[\"b_conv1\"] = tf.get_variable(name=\"b_conv1\", shape=layer_size[0][-1],\n",
    "                          initializer=tf.zeros_initializer())\n",
    "      \n",
    "      self.params[\"W_conv2\"] = tf.get_variable(name=\"W_conv2\", shape=layer_size[1],\n",
    "                          initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "      self.params[\"b_conv2\"] = tf.get_variable(name=\"b_conv2\", shape=layer_size[1][-1],\n",
    "                          initializer=tf.zeros_initializer())\n",
    "      \n",
    "      self.params[\"W_conv3\"] = tf.get_variable(name=\"W_conv3\", shape=layer_size[2],\n",
    "                          initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "      self.params[\"b_conv3\"] = tf.get_variable(name=\"b_conv3\", shape=layer_size[2][-1],\n",
    "                          initializer=tf.zeros_initializer())\n",
    "      \n",
    "      \n",
    "      self.params[\"W_fc1\"] = tf.get_variable(name=\"W_fc1\", shape=layer_size[3],\n",
    "                          initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "      self.params[\"b_fc1\"] = tf.get_variable(name=\"b_fc1\", shape=layer_size[3][-1],\n",
    "                          initializer=tf.zeros_initializer())      \n",
    "      \n",
    "      self.params[\"W_loc1\"] = tf.get_variable(name=\"W_loc1\", shape=(2, layer_size[3][-1]),\n",
    "                          initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "      self.params[\"b_loc1\"] = tf.get_variable(name=\"b_loc1\", shape=layer_size[3][-1],\n",
    "                          initializer=tf.zeros_initializer())\n",
    "    \n",
    "  def build(self, x, l, activation=tf.nn.relu):\n",
    "    \"\"\"Build glimpse network based on observation x and location l.\"\"\"\n",
    "    endpoints = {}\n",
    "    \n",
    "    # image network\n",
    "    # convolutional layer 1\n",
    "    endpoints[\"conv_layer1\"] = activation(\n",
    "      tf.nn.conv2d(\n",
    "        x, self.params[\"W_conv1\"], strides=(1, 1, 1, 1), padding='SAME'\n",
    "      ) + self.params[\"b_conv1\"])\n",
    "    \n",
    "    # convolutional layer 2\n",
    "    endpoints[\"conv_layer2\"] = activation(\n",
    "      tf.nn.conv2d(\n",
    "        endpoints[\"conv_layer1\"], self.params[\"W_conv2\"], strides=[1, 1, 1, 1], padding='SAME'\n",
    "      ) + self.params[\"b_conv2\"])\n",
    "    \n",
    "    # convolutional layer 3\n",
    "    endpoints[\"conv_layer3\"] = activation(\n",
    "      tf.nn.conv2d(\n",
    "        endpoints[\"conv_layer2\"], self.params[\"W_conv3\"], strides=[1, 1, 1, 1], padding='SAME'\n",
    "      ) + self.params[\"b_conv3\"])\n",
    "    \n",
    "    endpoints[\"conv_layer3_flattened\"] = tf.reshape(\n",
    "      endpoints[\"conv_layer3\"],\n",
    "      shape=(-1, np.prod(endpoints[\"conv_layer3\"].get_shape().as_list()[1:])))\n",
    "        \n",
    "    # fully connected layer\n",
    "    endpoints[\"fc_layer1\"] = activation(\n",
    "      tf.matmul(\n",
    "        endpoints[\"conv_layer3_flattened\"], self.params[\"W_fc1\"]) + self.params[\"b_fc1\"])\n",
    "    \n",
    "    # location network\n",
    "    endpoints[\"loc_layer1\"] = activation(\n",
    "      tf.matmul(l, self.params[\"W_loc1\"]) + self.params[\"b_loc1\"]\n",
    "    )\n",
    "    \n",
    "    # combined output\n",
    "    g = endpoints[\"fc_layer1\"] * endpoints[\"loc_layer1\"]\n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmissionNet(object):\n",
    "  \"\"\"Emission network.\n",
    "  \"\"\"\n",
    "  def __init__(self, layer_size=((128, 2),)):\n",
    "    \"\"\" \n",
    "    \n",
    "    Args:\n",
    "      layer_size: layer sizes (tuple of shape tuples).\n",
    "        default:\n",
    "          layer 1: fully connected, input with size 128, output size 2\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    self.params = {}\n",
    "    with tf.variable_scope(\"emission_net\"):\n",
    "      \n",
    "      self.params[\"W_fc1\"] = tf.get_variable(name=\"W_fc1\", shape=layer_size[0],\n",
    "                          initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "      self.params[\"b_fc1\"] = tf.get_variable(name=\"b_fc1\", shape=layer_size[0][-1],\n",
    "                          initializer=tf.zeros_initializer())      \n",
    "    \n",
    "  def build(self, r, activation=tf.nn.relu):\n",
    "    \"\"\"Build emission network based on rnn state r.\"\"\"\n",
    "    endpoints = {}\n",
    "    \n",
    "    # fully connected layer\n",
    "    endpoints[\"fc_layer1\"] = activation(\n",
    "      tf.matmul(\n",
    "        r, self.params[\"W_fc1\"]) + self.params[\"b_fc1\"])\n",
    "\n",
    "    l = endpoints[\"fc_layer1\"]\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(shape=(None, 16, 16, 1), dtype=tf.float32)\n",
    "l = tf.placeholder(shape=(None, 2), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
