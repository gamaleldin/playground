{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple object recognition with visual attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I implement the model from Ba et al. \"Multiple object recognition with visual attention\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials import mnist\n",
    "from tensorflow.python.ops import array_ops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 4  # times is the total number of pixels\n",
    "num_classes = 10 \n",
    "num_units = 128  # number of units in network\n",
    "max_iter = 50000\n",
    "init_lr = 0.001  # initial learning rate\n",
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mnist_provider(object):\n",
    "  \"\"\"MNIST data provider.\"\"\"\n",
    "  def __init__(self, data_directory, split=\"train\"):\n",
    "    mnist_data = mnist.input_data.read_data_sets(data_directory, one_hot=True)\n",
    "    if split == \"train\":\n",
    "      self.mnist_data = mnist_data.train\n",
    "    elif split == \"valid\":\n",
    "      self.mnist_data = mnist_data.validation\n",
    "    elif split == \"test\":\n",
    "      self.mnist_data = mnist_data.test\n",
    "\n",
    "  def next_batch(self, batch_size):\n",
    "    images, one_hot_labels = self.mnist_data.next_batch(batch_size)\n",
    "    images = np.reshape(images, [-1, 28, 28, 1], order='C')\n",
    "    return images, one_hot_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlimpseNet(object):\n",
    "  \"\"\"Glimpse network.\n",
    "  \n",
    "  \"The glimpse network is a non-linear function that receives the current input image\n",
    "  patch, or glimpse, xn and its location tuple ln , where ln = (xn, yn), as input and \n",
    "  outputs a vector gn\"\n",
    "  \"\"\"\n",
    "  def __init__(self, \n",
    "               layers_size=(\n",
    "                 (3, 3, 1, 64),\n",
    "                 (3, 3, 64, 64),\n",
    "                 (3, 3, 64, 64),\n",
    "                 (16 * 16 * 64, num_units)\n",
    "               )):\n",
    "    \"\"\" \n",
    "    \n",
    "    Args:\n",
    "      layers_size: layer sizes (tuple of shape tuples).\n",
    "        default:\n",
    "          layer 1: 3x3 conv, input with 1 color channel, 64 channels output\n",
    "          layer 2: 3x3 conv, 64 channels output\n",
    "          layer 3: 3x3 conv, 64 channels output\n",
    "          layer 4: fully connected, input with size 16x16, output size 128\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    self.params = {}\n",
    "    with tf.variable_scope(\"glimpse_net\"):\n",
    "      self.params[\"W_conv1\"] = tf.get_variable(name=\"W_conv1\", shape=layers_size[0],\n",
    "                          initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "      self.params[\"b_conv1\"] = tf.get_variable(name=\"b_conv1\", shape=layers_size[0][-1],\n",
    "                          initializer=tf.zeros_initializer())\n",
    "      \n",
    "      self.params[\"W_conv2\"] = tf.get_variable(name=\"W_conv2\", shape=layers_size[1],\n",
    "                          initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "      self.params[\"b_conv2\"] = tf.get_variable(name=\"b_conv2\", shape=layers_size[1][-1],\n",
    "                          initializer=tf.zeros_initializer())\n",
    "      \n",
    "      self.params[\"W_conv3\"] = tf.get_variable(name=\"W_conv3\", shape=layers_size[2],\n",
    "                          initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "      self.params[\"b_conv3\"] = tf.get_variable(name=\"b_conv3\", shape=layers_size[2][-1],\n",
    "                          initializer=tf.zeros_initializer())\n",
    "      \n",
    "      \n",
    "      self.params[\"W_fc1\"] = tf.get_variable(name=\"W_fc1\", shape=layers_size[3],\n",
    "                          initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "      self.params[\"b_fc1\"] = tf.get_variable(name=\"b_fc1\", shape=layers_size[3][-1],\n",
    "                          initializer=tf.zeros_initializer())      \n",
    "      \n",
    "      self.params[\"W_loc1\"] = tf.get_variable(name=\"W_loc1\", shape=(2, layers_size[3][-1]),\n",
    "                          initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "      self.params[\"b_loc1\"] = tf.get_variable(name=\"b_loc1\", shape=layers_size[3][-1],\n",
    "                          initializer=tf.zeros_initializer())\n",
    "    \n",
    "  def build(self, x_l, l, activation=tf.nn.relu):\n",
    "    \"\"\"Build glimpse network based on observation x_l at location l and location l.\"\"\"\n",
    "    endpoints = {}\n",
    "    with tf.name_scope(\"glimpse_net\"):\n",
    "      # image network\n",
    "      # convolutional layer 1\n",
    "      endpoints[\"conv_layer1\"] = activation(\n",
    "        tf.nn.conv2d(\n",
    "          x_l, self.params[\"W_conv1\"], strides=(1, 1, 1, 1), padding='SAME'\n",
    "        ) + self.params[\"b_conv1\"])\n",
    "\n",
    "      # convolutional layer 2\n",
    "      endpoints[\"conv_layer2\"] = activation(\n",
    "        tf.nn.conv2d(\n",
    "          endpoints[\"conv_layer1\"], self.params[\"W_conv2\"], strides=[1, 1, 1, 1], padding='SAME'\n",
    "        ) + self.params[\"b_conv2\"])\n",
    "\n",
    "      # convolutional layer 3\n",
    "      endpoints[\"conv_layer3\"] = activation(\n",
    "        tf.nn.conv2d(\n",
    "          endpoints[\"conv_layer2\"], self.params[\"W_conv3\"], strides=[1, 1, 1, 1], padding='SAME'\n",
    "        ) + self.params[\"b_conv3\"])\n",
    "\n",
    "      endpoints[\"conv_layer3_flattened\"] = tf.reshape(\n",
    "        endpoints[\"conv_layer3\"],\n",
    "        shape=(-1, np.prod(endpoints[\"conv_layer3\"].get_shape().as_list()[1:])))\n",
    "\n",
    "      # fully connected layer\n",
    "      endpoints[\"fc_layer1\"] = activation(\n",
    "        tf.matmul(\n",
    "          endpoints[\"conv_layer3_flattened\"], self.params[\"W_fc1\"]) + self.params[\"b_fc1\"])\n",
    "\n",
    "      # location network\n",
    "      endpoints[\"loc_layer1\"] = activation(\n",
    "        tf.matmul(l, self.params[\"W_loc1\"]) + self.params[\"b_loc1\"]\n",
    "      )\n",
    "\n",
    "      # combined output\n",
    "      g = endpoints[\"fc_layer1\"] * endpoints[\"loc_layer1\"]\n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmissionNet(object):\n",
    "  \"\"\"Emission network.\n",
    "  \n",
    "  \"The emission network takes the current state of recurrent network as input and \n",
    "  makes a prediction on where to extract the next image patch for the glimpse network.\"\n",
    "  \n",
    "  \"\"\"\n",
    "  def __init__(self, layers_size=((num_units, 2),)):\n",
    "    \"\"\" \n",
    "    \n",
    "    Args:\n",
    "      layers_size: layer sizes (tuple of shape tuples).\n",
    "        default:\n",
    "          layer 1: fully connected, input with size 128, output size 2\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    self.params = {}\n",
    "    with tf.variable_scope(\"emission_net\"):\n",
    "      \n",
    "      self.params[\"W_fc1\"] = tf.get_variable(name=\"W_fc1\", shape=layers_size[0],\n",
    "                          initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "      self.params[\"b_fc1\"] = tf.get_variable(name=\"b_fc1\", shape=layers_size[0][-1],\n",
    "                          initializer=tf.zeros_initializer())      \n",
    "    \n",
    "  def build(self, r, activation=tf.nn.relu):\n",
    "    \"\"\"Build emission network based on rnn state r.\"\"\"\n",
    "    endpoints = {}\n",
    "    \n",
    "    with tf.name_scope(\"emission_net\"):\n",
    "      # fully connected layer\n",
    "      endpoints[\"fc_layer1\"] = activation(\n",
    "        tf.matmul(\n",
    "          r, self.params[\"W_fc1\"]) + self.params[\"b_fc1\"])\n",
    "\n",
    "      # normalized location (-1, 1) range. [0, 0] is center.\n",
    "      l = 2 * tf.nn.sigmoid(endpoints[\"fc_layer1\"]) - 1\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextNet(object):\n",
    "  \"\"\"Context network.\n",
    "  \n",
    "  \"The context network provides the initial state for the recurrent network and its\n",
    "  output is used by the emission network to predict the location of the first glimpse. The context\n",
    "  network C(Â·) takes a down-sampled low-resolution version of the whole input image Icoarse and\n",
    "  outputs a fixed length vector cI . The contextual information provides sensible hints on where the\n",
    "  potentially interesting regions are in a given image.\"\n",
    "  \"\"\"\n",
    "  def __init__(self, \n",
    "               layers_size=(\n",
    "                 (3, 3, 1, 64),\n",
    "                 (3, 3, 64, 64),\n",
    "                 (3, 3, 64, 64),\n",
    "                 (16 * 16 * 64, num_units)\n",
    "               )):\n",
    "    self.params = {}\n",
    "    with tf.variable_scope(\"context_net\"):\n",
    "      self.params[\"W_conv1\"] = tf.get_variable(name=\"W_conv1\", shape=layers_size[0],\n",
    "                          initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "      self.params[\"b_conv1\"] = tf.get_variable(name=\"b_conv1\", shape=layers_size[0][-1],\n",
    "                          initializer=tf.zeros_initializer())\n",
    "      \n",
    "      self.params[\"W_conv2\"] = tf.get_variable(name=\"W_conv2\", shape=layers_size[1],\n",
    "                          initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "      self.params[\"b_conv2\"] = tf.get_variable(name=\"b_conv2\", shape=layers_size[1][-1],\n",
    "                          initializer=tf.zeros_initializer())\n",
    "      \n",
    "      self.params[\"W_conv3\"] = tf.get_variable(name=\"W_conv3\", shape=layers_size[2],\n",
    "                          initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "      self.params[\"b_conv3\"] = tf.get_variable(name=\"b_conv3\", shape=layers_size[2][-1],\n",
    "                          initializer=tf.zeros_initializer())\n",
    "      \n",
    "      \n",
    "      self.params[\"W_fc1\"] = tf.get_variable(name=\"W_fc1\", shape=layers_size[3],\n",
    "                          initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "      self.params[\"b_fc1\"] = tf.get_variable(name=\"b_fc1\", shape=layers_size[3][-1],\n",
    "                          initializer=tf.zeros_initializer())      \n",
    "\n",
    "  \n",
    "  def build(self, x_coarse, activation=tf.nn.relu):\n",
    "    \"\"\"Build context network.\"\"\"\n",
    "    endpoints = {}\n",
    "    with tf.name_scope(\"context_net\"):\n",
    "      # image network\n",
    "      # convolutional layer 1\n",
    "      endpoints[\"conv_layer1\"] = activation(\n",
    "        tf.nn.conv2d(\n",
    "          x_coarse, self.params[\"W_conv1\"], strides=(1, 1, 1, 1), padding='SAME'\n",
    "        ) + self.params[\"b_conv1\"])\n",
    "\n",
    "      # convolutional layer 2\n",
    "      endpoints[\"conv_layer2\"] = activation(\n",
    "        tf.nn.conv2d(\n",
    "          endpoints[\"conv_layer1\"], self.params[\"W_conv2\"], strides=[1, 1, 1, 1], padding='SAME'\n",
    "        ) + self.params[\"b_conv2\"])\n",
    "\n",
    "      # convolutional layer 3\n",
    "      endpoints[\"conv_layer3\"] = activation(\n",
    "        tf.nn.conv2d(\n",
    "          endpoints[\"conv_layer2\"], self.params[\"W_conv3\"], strides=[1, 1, 1, 1], padding='SAME'\n",
    "        ) + self.params[\"b_conv3\"])\n",
    "\n",
    "      endpoints[\"conv_layer3_flattened\"] = tf.reshape(\n",
    "        endpoints[\"conv_layer3\"],\n",
    "        shape=(-1, np.prod(endpoints[\"conv_layer3\"].get_shape().as_list()[1:])))\n",
    "\n",
    "      # fully connected layer\n",
    "      endpoints[\"fc_layer1\"] = activation(\n",
    "        tf.matmul(\n",
    "          endpoints[\"conv_layer3_flattened\"], self.params[\"W_fc1\"]) + self.params[\"b_fc1\"])\n",
    "      \n",
    "    return endpoints[\"fc_layer1\"]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DRAM(object):\n",
    "  \"\"\"Class for deep recurrent attention model (DRAM).\n",
    "  \n",
    "  Args:\n",
    "    weights: dictionary of weights as tensorflow variables.\n",
    "    biases: dictionary of biases as tensorflow variables.\n",
    "    outputs: rnn output pre logit (list of size number of time steps).\n",
    "    state: final rnn state.\n",
    "    cell: tensorflow cell obbject\n",
    "    logits_list: list of logits at each time point.\n",
    "  \"\"\"\n",
    "  def __init__(self, images, num_units, num_classes, time_steps, activation=tf.nn.tanh):\n",
    "    \"\"\"Init function.\n",
    "    \n",
    "    Inputs:\n",
    "      images: model input.\n",
    "      num_units: number of network units.\n",
    "      num_classes: network output classes.\n",
    "      time_steps: number of time steps for RNNS\n",
    "      activation: activation function to use for cell (default tanh).\n",
    "    \"\"\"\n",
    "    self.weights = {}\n",
    "    self.biases = {}\n",
    "    \n",
    "    # initial states \n",
    "    # context information\n",
    "    glimpse_size = coarse_size = (16, 16)\n",
    "    C = ContextNet()\n",
    "    images_coarse = tf.image.resize_images(images, coarse_size)\n",
    "    h0 = C.build(images_coarse)\n",
    "    \n",
    "    # initial location\n",
    "    E = EmissionNet()\n",
    "    l0 = E.build(h0)\n",
    "    \n",
    "    # first glimpse\n",
    "    G = GlimpseNet()\n",
    "\n",
    "    #TODO(gamaleldin): change GRU to LSTM\n",
    "    with tf.variable_scope(\"rnn_layer1\"):\n",
    "      with tf.variable_scope(\"internal\"):\n",
    "        self.rnn_layer1 = tf.nn.rnn_cell.GRUCell(\n",
    "          num_units=num_units, activation=activation)\n",
    "        \n",
    "        self.rnn_layer2 = tf.nn.rnn_cell.GRUCell(\n",
    "          num_units=num_units, activation=activation)\n",
    "        \n",
    "      \n",
    "      state_layer1 = self.rnn_layer1.zero_state(\n",
    "        batch_size=array_ops.shape(images)[0],\n",
    "        dtype=tf.float32)\n",
    "      \n",
    "      state_layer2 = h0\n",
    "      self.outputs1 = []\n",
    "      self.states1 = []\n",
    "      \n",
    "      self.outputs2 = []\n",
    "      self.states2 = []\n",
    "      l = l0\n",
    "      for t in range(time_steps):\n",
    "        with tf.variable_scope(\"internal\", reuse=(t != 0)):\n",
    "          images_glimpse0 = tf.image.extract_glimpse(\n",
    "            images,\n",
    "            size=glimpse_size,\n",
    "            offsets=l,\n",
    "            centered=True,\n",
    "            normalized=True)\n",
    "          g = G.build(images_glimpse0, l)\n",
    "          \n",
    "          output1, state_layer1 = self.rnn_layer1(g, state_layer1)\n",
    "          self.outputs1.append(output1)\n",
    "          self.states1.append(state_layer1) \n",
    "          \n",
    "          output2, state_layer2 = self.rnn_layer1(output1, state_layer2)\n",
    "          self.outputs2.append(output2)\n",
    "          self.states2.append(state_layer2) \n",
    "            \n",
    "          l = E.build(output2)\n",
    "          \n",
    "      with tf.variable_scope(\"output\"):\n",
    "        self.weights[\"out\"] = tf.get_variable(name=\"w\",\n",
    "                        shape=(num_units, num_classes),\n",
    "                        initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        \n",
    "        self.biases[\"out\"] = tf.get_variable(name=\"b\",\n",
    "                        shape=(num_classes),\n",
    "                        initializer=tf.zeros_initializer())\n",
    "\n",
    "    # compute logits at each time step\n",
    "    self.logits_list = []\n",
    "    for output in self.outputs1:\n",
    "      self.logits_list.append(\n",
    "        tf.matmul(output, self.weights[\"out\"]) + self.biases[\"out\"]) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-650099426339>:4: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /Users/gamal/git_local_repo/playground/data/mnist/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /Users/gamal/git_local_repo/playground/data/mnist/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting /Users/gamal/git_local_repo/playground/data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting /Users/gamal/git_local_repo/playground/data/mnist/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: __init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "Extracting /Users/gamal/git_local_repo/playground/data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting /Users/gamal/git_local_repo/playground/data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting /Users/gamal/git_local_repo/playground/data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting /Users/gamal/git_local_repo/playground/data/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Extracting /Users/gamal/git_local_repo/playground/data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting /Users/gamal/git_local_repo/playground/data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting /Users/gamal/git_local_repo/playground/data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting /Users/gamal/git_local_repo/playground/data/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "data_dir = \"/Users/gamal/git_local_repo/playground/data/mnist\"\n",
    "data_provider_train = mnist_provider(data_dir, split='train')\n",
    "data_provider_valid = mnist_provider(data_dir, split='valid')\n",
    "data_provider_test = mnist_provider(data_dir, split='test')\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "  # place golder for data\n",
    "  images = tf.placeholder(shape=(None, 28, 28, 1), dtype=tf.float32)\n",
    "  one_hot_labels = tf.placeholder(dtype=tf.float32, shape=(None, num_classes))\n",
    "  \n",
    "  # build DRAM model\n",
    "  D = DRAM(images, num_units=num_units, num_classes=num_classes, time_steps=T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with g.as_default():\n",
    "  logits = D.logits_list[-1]\n",
    "  \n",
    "  # accuracy metric\n",
    "  top1_op = tf.nn.in_top_k(logits, tf.argmax(one_hot_labels, 1), 1)\n",
    "  accuracy = tf.reduce_mean(tf.cast(top1_op, dtype=tf.float32))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with g.as_default():\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,\n",
    "                                               labels=one_hot_labels)\n",
    "  )\n",
    "  opt = tf.train.AdamOptimizer(learning_rate=init_lr)\n",
    "  \n",
    "  train_op = opt.minimize(loss)\n",
    "  \n",
    "  init_op = tf.global_variables_initializer()\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with g.as_default():\n",
    "  saver = tf.train.Saver(tf.global_variables())\n",
    "  train_dir = \"/Users/gamal/git_local_repo/playground/experiments/visual_attention/train\"\n",
    "  if not os.path.exists(train_dir):\n",
    "    os.mkdir(train_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: (train loss: 2.303) (train accuracy: 0.570)\n"
     ]
    }
   ],
   "source": [
    "with g.as_default():\n",
    "  with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    for i in xrange(max_iter):\n",
    "      imgs, lbls = data_provider_train.next_batch(\n",
    "        batch_size=batch_size)\n",
    "      _, ls, acc = sess.run(\n",
    "        (train_op, loss, accuracy),\n",
    "        feed_dict={images: imgs, one_hot_labels: lbls}\n",
    "      )\n",
    "      if i % 100 == 0:\n",
    "        saver.save(sess, os.path.join(train_dir, 'dram_model.ckpt'), global_step=i)\n",
    "        print(\n",
    "          \"iter %d: (train loss: %.3f) (train accuracy: %.3f)\" \n",
    "          %(i, ls, acc)\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
